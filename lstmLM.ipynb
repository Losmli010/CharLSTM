{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    values = np.exp(x - np.max(x))\n",
    "    return values/ np.sum(values)\n",
    "\n",
    "def save_model_parameters(model, outfile):\n",
    "    np.savez(outfile,\n",
    "        Wf=model.Wf,\n",
    "        Uf=model.Uf,\n",
    "        bf=model.bf,\n",
    "        Wi=model.Wi,\n",
    "        Ui=model.Ui,\n",
    "        bi=model.bi,\n",
    "        Wg=model.Wg,\n",
    "        Ug=model.Ug,\n",
    "        bg=model.bg,\n",
    "        Wo=model.Wo,\n",
    "        Uo=model.Uo,\n",
    "        bo=model.bo,\n",
    "        V=model.V,\n",
    "        b=model.b)\n",
    "    print(\"Saved model parameters to %s.\" % outfile)\n",
    "\n",
    "def load_model_parameters(path, modelClass=LSTMLM):\n",
    "    npzfile = np.load(path)\n",
    "    Wf,Uf,bf,Wi,Ui,bi,Wg,Ug,bg,Wo,Uo,bo,V,b = npzfile[\"Wf\"], npzfile[\"Uf\"], npzfile[\"bf\"], npzfile[\"Wi\"], npzfile[\"Ui\"], npzfile[\"bi\"],npzfile[\"Wg\"], npzfile[\"Ug\"], npzfile[\"bg\"], npzfile[\"Wo\"], npzfile[\"Uo\"], npzfile[\"bo\"],npzfile['V'],npzfile['b']\n",
    "    hidden_dim, word_dim = Wf.shape\n",
    "    print(\"Building model model from %s with hidden_dim=%d word_dim=%d\" % (path, hidden_dim, word_dim))\n",
    "    sys.stdout.flush()\n",
    "    model = modelClass(word_dim, hidden_dim=hidden_dim)\n",
    "    model.Wf=Wf,\n",
    "    model.Uf=Uf,\n",
    "    model.bf=bf,\n",
    "    model.Wi=Wi,\n",
    "    model.Ui=Ui,\n",
    "    model.bi=bi,\n",
    "    model.Wg=Wg,\n",
    "    model.Ug=Ug,\n",
    "    model.bg=bg,\n",
    "    model.Wo=Wo,\n",
    "    model.Uo=Uo,\n",
    "    model.bo=bo,\n",
    "    model.V=V,\n",
    "    model.b=b\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LSTMLM:\n",
    "    def __init__(self,word_dim,hidden_dim=128,bptt_truncate=-1):\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        \n",
    "        self.Wf = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, word_dim))\n",
    "        self.Uf = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, hidden_dim))\n",
    "        self.bf=np.zeros(hidden_dim)\n",
    "        self.Wi = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, word_dim))\n",
    "        self.Ui = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, hidden_dim))\n",
    "        self.bi=np.zeros(hidden_dim)\n",
    "        self.Wg = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, word_dim))\n",
    "        self.Ug = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, hidden_dim))\n",
    "        self.bg=np.zeros(hidden_dim)\n",
    "        self.Wo = np.random.uniform(-np.sqrt(1. / hidden_dim), np.sqrt(1. / hidden_dim), (hidden_dim, word_dim))\n",
    "        self.Uo = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (hidden_dim, hidden_dim))\n",
    "        self.bo=np.zeros(hidden_dim)\n",
    "        self.V = np.random.uniform(-np.sqrt(1. / word_dim), np.sqrt(1. / word_dim), (word_dim, hidden_dim))\n",
    "        self.b=np.zeros(word_dim)\n",
    "        \n",
    "    \n",
    "    def forward_propagation(self,x):\n",
    "        T=len(x)\n",
    "        f=np.zeros((T,self.hidden_dim))\n",
    "        i=np.zeros((T,self.hidden_dim))\n",
    "        g=np.zeros((T,self.hidden_dim))\n",
    "        c=np.zeros((T+1,self.hidden_dim))\n",
    "        o=np.zeros((T,self.hidden_dim))\n",
    "        h=np.zeros((T+1,self.hidden_dim))\n",
    "        z=np.zeros((T,self.word_dim))\n",
    "        \n",
    "        for t in np.arange(T):\n",
    "            inputs=np.zeros(self.word_dim)\n",
    "            inputs[x[t]]=1\n",
    "            f[t]=sigmoid(np.dot(self.Wf,inputs)+np.dot(self.Uf,h[t-1])+self.bf)\n",
    "            i[t]=sigmoid(np.dot(self.Wi,inputs)+np.dot(self.Ui,h[t-1])+self.bi)\n",
    "            g[t]=np.tanh(np.dot(self.Wg,inputs)+np.dot(self.Ug,h[t-1])+self.bg)\n",
    "            c[t]=f[t]*c[t-1]+i[t]*g[t]\n",
    "            o[t]=sigmoid(np.dot(self.Wo,inputs)+np.dot(self.Uo,h[t-1])+self.bo)\n",
    "            h[t]=o[t]*np.tanh(c[t])\n",
    "            z[t] = softmax(np.dot(self.V,h[t])+self.b)\n",
    "        return (f,i,g,c,o,h,z)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        f,i,g,c,o,h,z=self.forward_propagation(x)\n",
    "        return np.argmax(z,axis=1)\n",
    "    \n",
    "    def calculate_loss(self,x,y):\n",
    "        f,i,g,c,o,h,z=self.forward_propagation(x)\n",
    "        loss=0.0\n",
    "        for i in np.arange(len(y)):\n",
    "            correct_word_predictions = z[i, y[i]]\n",
    "            loss+=-1.0*np.log(correct_word_predictions)\n",
    "        return loss\n",
    "    \n",
    "    def calculate_total_loss(self,X,Y):\n",
    "        L=0.0\n",
    "        N=0\n",
    "        for i in np.arange(len(Y)):\n",
    "            L+=self.calculate_loss(X[i],Y[i])\n",
    "            N+=len(Y[i])\n",
    "        return L/N\n",
    "    \n",
    "    def bptt(self, x, y):\n",
    "        T = len(y)\n",
    "        f,i,g,c,o,h,z = self.forward_propagation(x)\n",
    "    \n",
    "        dWf = np.zeros(self.Wf.shape)\n",
    "        dUf = np.zeros(self.Uf.shape)\n",
    "        dbf = np.zeros(self.bf.shape)\n",
    "        dWi = np.zeros(self.Wi.shape)\n",
    "        dUi = np.zeros(self.Ui.shape)\n",
    "        dbi = np.zeros(self.bi.shape)\n",
    "        dWg = np.zeros(self.Wg.shape)\n",
    "        dUg = np.zeros(self.Ug.shape)\n",
    "        dbg = np.zeros(self.bg.shape)\n",
    "        dWo = np.zeros(self.Wo.shape)\n",
    "        dUo = np.zeros(self.Uo.shape)\n",
    "        dbo = np.zeros(self.bo.shape)\n",
    "        dV = np.zeros(self.V.shape)\n",
    "        db = np.zeros(self.b.shape)\n",
    "        \n",
    "        delta_z = z\n",
    "        delta_z[np.arange(len(y)), y] -= 1.0\n",
    "        \n",
    "        delta_h=np.zeros(h.shape)\n",
    "        delta_c=np.zeros(c.shape)\n",
    "        for t in np.arange(T)[::-1]:\n",
    "            dV += np.outer(delta_z[t], h[t].T)\n",
    "            db += delta_z[t]\n",
    "\n",
    "            delta_h[t] = np.dot(self.V.T,delta_z[t])+delta_h[t+1]\n",
    "            delta_o = delta_h[t]*np.tanh(c[t])\n",
    "            delta_c[t] = delta_h[t]*o[t]*(1-np.tanh(c[t])**2)+delta_c[t+1]\n",
    "            \n",
    "            delta_i=delta_c[t]*g[t]*i[t]*(1-i[t])\n",
    "            delta_g=delta_c[t]*i[t]*(1-g[t]**2)\n",
    "            delta_f=delta_c[t]*c[t-1]*f[t]*(1-f[t])\n",
    "            delta_o_net=delta_o*o[t]*(1-o[t])\n",
    "                    \n",
    "            inputs=np.zeros(self.word_dim)\n",
    "            inputs[x[t]]=1\n",
    "            dWf +=np.outer(delta_f,inputs.T)\n",
    "            dUf +=np.outer(delta_f,h[t-1].T)\n",
    "            dbf +=delta_f\n",
    "            dWi +=np.outer(delta_i,inputs.T)\n",
    "            dUi +=np.outer(delta_i,h[t-1].T)\n",
    "            dbi +=delta_i\n",
    "            dWg +=np.outer(delta_g,inputs.T)\n",
    "            dUg +=np.outer(delta_g,h[t-1].T)\n",
    "            dbg +=delta_g\n",
    "            dWo +=np.outer(delta_o_net,inputs.T)\n",
    "            dUo +=np.outer(delta_o_net,h[t-1].T)\n",
    "            dbo +=delta_o_net\n",
    "           \n",
    "        return (dWf,dUf,dbf,dWi,dUi,dbi,dWg,dUg,dbg,dWo,dUo,dbo,dV,db)\n",
    "    \n",
    "    def sgd_step(self, x, y, learning_rate):\n",
    "        dWf,dUf,dbf,dWi,dUi,dbi,dWg,dUg,dbg,dWo,dUo,dbo,dV,db = self.bptt(x, y)\n",
    "        self.Wf -= learning_rate * dWf\n",
    "        self.Uf -= learning_rate * dUf\n",
    "        self.bf -= learning_rate * dbf\n",
    "        self.Wi -= learning_rate * dWi\n",
    "        self.Ui -= learning_rate * dUi\n",
    "        self.bi -= learning_rate * dbi\n",
    "        self.Wg -= learning_rate * dWg\n",
    "        self.Ug -= learning_rate * dUg\n",
    "        self.bg -= learning_rate * dbg\n",
    "        self.Wo -= learning_rate * dWo\n",
    "        self.Uo -= learning_rate * dUo\n",
    "        self.bo -= learning_rate * dbo\n",
    "        self.V -= learning_rate * dV\n",
    "        self.b -= learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "def train(model, X, Y, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    num_examples_seen = 0\n",
    "    losses = []\n",
    "    for epoch in range(nepoch):\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_total_loss(X, Y)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print(\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if len(losses) > 1 and losses[-1][1] > losses[-2][1]:\n",
    "                learning_rate = learning_rate * 0.5\n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(Y)):\n",
    "            model.sgd_step(X[i], Y[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "        if epoch%100==0:\n",
    "            save_model_parameters(model, 'lstmlm.parameters.epoch%s'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CSV file...\n",
      "Parsed 79171 sentences.\n",
      "Found 65467 unique words tokens.\n",
      "Using vocabulary size 8000.\n",
      "The least frequent word in our vocabulary is 'documentary' and appeared 10 times.\n",
      "\n",
      "Example sentence: 'SENTENCE_START i joined a new league this year and they have different scoring rules than i'm used to. SENTENCE_END'\n",
      "\n",
      "Example sentence after Pre-processing: '['SENTENCE_START', 'i', 'joined', 'a', 'new', 'league', 'this', 'year', 'and', 'they', 'have', 'different', 'scoring', 'rules', 'than', 'i', \"'m\", 'used', 'to', '.', 'SENTENCE_END']'\n",
      "\n",
      "X_train shape: (78483,)\n",
      "y_train shape: (78483,)\n",
      "x:\n",
      "SENTENCE_START what are n't you understanding about this ? !\n",
      "[0, 51, 27, 16, 10, 857, 54, 25, 34, 69]\n",
      "\n",
      "y:\n",
      "what are n't you understanding about this ? ! SENTENCE_END\n",
      "[51, 27, 16, 10, 857, 54, 25, 34, 69, 1]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import nltk\n",
    "\n",
    "def getSentenceData(path, vocabulary_size=8000):\n",
    "    unknown_token = \"UNKNOWN_TOKEN\"\n",
    "    sentence_start_token = \"SENTENCE_START\"\n",
    "    sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "    # Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
    "    print(\"Reading CSV file...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, skipinitialspace=True)\n",
    "        # Split full comments into sentences\n",
    "        sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "        # Append SENTENCE_START and SENTENCE_END\n",
    "        sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "    print(\"Parsed %d sentences.\" % (len(sentences)))\n",
    "\n",
    "    # Tokenize the sentences into words\n",
    "    tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    # Filter the sentences having few words (including SENTENCE_START and SENTENCE_END)\n",
    "    tokenized_sentences = list(filter(lambda x: len(x) > 3, tokenized_sentences))\n",
    "\n",
    "    # Count the word frequencies\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "\n",
    "    # Get the most common words and build index_to_word and word_to_index vectors\n",
    "    vocab = word_freq.most_common(vocabulary_size-1)\n",
    "    index_to_word = [x[0] for x in vocab]\n",
    "    index_to_word.append(unknown_token)\n",
    "    word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "    print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "    print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
    "\n",
    "    # Replace all words not in our vocabulary with the unknown token\n",
    "    for i, sent in enumerate(tokenized_sentences):\n",
    "        tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "\n",
    "    print(\"\\nExample sentence: '%s'\" % sentences[1])\n",
    "    print(\"\\nExample sentence after Pre-processing: '%s'\\n\" % tokenized_sentences[0])\n",
    "\n",
    "    # Create the training data\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "    print(\"X_train shape: \" + str(X_train.shape))\n",
    "    print(\"y_train shape: \" + str(y_train.shape))\n",
    "\n",
    "    # Print an training data example\n",
    "    x_example, y_example = X_train[17], y_train[17]\n",
    "    print(\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "    print(\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))\n",
    "\n",
    "    return X_train, y_train,index_to_word, word_to_index\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X_train, y_train ,index_to_word, word_to_index= getSentenceData('data/reddit-comments-2015-08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm=LSTMLM(8000)\n",
    "train(lstm, X_train, y_train, learning_rate=0.005, nepoch=1001, evaluate_loss_after=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sentence(model, index_to_word, word_to_index):\n",
    "    new_sentence = [word_to_index['SENTENCE_START']]\n",
    "    while not (new_sentence[-1] == word_to_index['SENTENCE_END'] or len(new_sentence) > 100 or new_sentence[-1]==word_to_index['UNKNOWN_TOKEN']):\n",
    "        next_word = model.predict(new_sentence)[-1]\n",
    "        new_sentence.append(next_word)\n",
    "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
    "    return \" \".join(sentence_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model model from lstmlm.parameters.epoch 0.npz with hidden_dim=128 word_dim=8000\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n",
      "thethethethethethethethethe\n"
     ]
    }
   ],
   "source": [
    "model=load_model_parameters('lstmlm.parameters.epoch 0.npz')\n",
    "sent=generate_sentence(model,index_to_word, word_to_index)\n",
    "for i in range(10):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
